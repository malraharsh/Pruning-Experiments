{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pruning Experiments-on-CIFAR",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malraharsh/Pruning-Experiments/blob/master/Pruning_Experiments_on_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJwIonXEVJo6",
        "colab": {}
      },
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# %load_ext tensorboard\n",
        "\n",
        "if not os.path.exists('log'):\n",
        "    os.mkdir('log')\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "SHOW = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbY-KGMPvbW9",
        "colab": {}
      },
      "source": [
        "data = datasets.cifar10.load_data()\n",
        "\n",
        "(Train_images, Train_labels), (Test_images, Test_labels) = data\n",
        "\n",
        "# pct_data = 0.1\n",
        "# top = int(np.ceil(Train_images.shape[0] * pct_data))\n",
        "\n",
        "# (train_images, train_labels), (test_images, test_labels) = (Train_images[:top], Train_labels[:top]), (Test_images[:top], Test_labels[:top])\n",
        "# print(top)\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsf6QDBAAIa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_labels = test_images = test_labels = None\n",
        "\n",
        "def change_pct_data(pct_data):    \n",
        "    global train_images, train_labels, test_images, test_labels    \n",
        "    top = int(np.ceil(Train_images.shape[0] * pct_data))\n",
        "    (train_images, train_labels), (test_images, test_labels) = (Train_images[:top], Train_labels[:top]), (Test_images[:top], Test_labels[:top])\n",
        "    print(f\"No of data - {top}\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB25l8eRC6oH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_images, train_labels, test_images, test_labels):\n",
        "\n",
        "    # Normalize the input image so that each pixel value is between 0 to 1.\n",
        "    train_images = train_images.copy() / 255.0 #!!!!! CAN REOMVE COPY\n",
        "    test_images = test_images.copy() / 255.0\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_images, train_labels, epochs=EPOCHS, \n",
        "                        validation_data=(test_images, test_labels), verbose=VERBOSE)\n",
        "\n",
        "    #no test, only val data\n",
        "    # test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "    # history.history['test_loss'] = [test_loss]\n",
        "    # history.history['test_accuracy'] = [test_accuracy]\n",
        "\n",
        "    if SHOW:\n",
        "\n",
        "        plt.plot(history.history['accuracy'], label='accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.ylim([0.5, 1])\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "    \n",
        "    return model, history.history\n",
        "    \n",
        "\n",
        "def prune(train_images, train_labels, model):\n",
        "\n",
        "    # Compute end step to finish pruning after 2 epochs.\n",
        "    batch_size = 128\n",
        "    epochs = EPOCHS_PRUNE\n",
        "    validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "    num_images = train_images.shape[0] * (1 - validation_split)\n",
        "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "    # Define model for pruning.\n",
        "    pruning_params = {\n",
        "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                                   final_sparsity=0.80,\n",
        "                                                                   begin_step=0,\n",
        "                                                                   end_step=end_step)\n",
        "    }\n",
        "\n",
        "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "    # `prune_low_magnitude` requires a recompile.\n",
        "    model_for_pruning.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model_for_pruning.summary()\n",
        "\n",
        "    callbacks = [\n",
        "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    #   tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "    ]\n",
        "\n",
        "    history = model_for_pruning.fit(train_images, train_labels,\n",
        "                      batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_labels),\n",
        "                      callbacks=callbacks, verbose=VERBOSE)\n",
        "    \n",
        "    # test_loss, test_accuracy = model_for_pruning.evaluate(test_images, test_labels, verbose=0)\n",
        "    # history.history['test_loss'] = [test_loss]\n",
        "    # history.history['test_accuracy'] = [test_accuracy]\n",
        "\n",
        "    if SHOW:\n",
        "        print('Pruned test accuracy:', test_accuracy)\n",
        "\n",
        "    return model_for_pruning, history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIyrGM-rBkkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_train(test_pct):\n",
        "    train_x, val_x, train_y, val_y = train_test_split(train_images, train_labels, test_size=test_pct, stratify=train_labels)\n",
        "    \n",
        "    print(\"TRAINING ---\")\n",
        "    model, info_train = train(train_x, train_y, val_x, val_y)\n",
        "    \n",
        "    print(\"PRUNING ---\")\n",
        "    _, info_prune = prune(train_x, train_y, model)\n",
        "    \n",
        "    return info_train, info_prune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RtTs_t9Bkfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_info(dic, pct_train, info):\n",
        "    # print(list(dic.items()), '----')\n",
        "    dic = {k:v[-1] for k, v in dic.items()}\n",
        "    dic['percentage'] = pct_train\n",
        "    return info.append(dic, ignore_index=True)   \n",
        "\n",
        "def save(df, name, pct_data):\n",
        "    df.to_csv(f'log/info-{name}-{pct_data*100}%.csv')\n",
        "\n",
        "\n",
        "# def savefile():\n",
        "#     df_info_train = add_info(info_train, 1 - pct_test, df_info_train)\n",
        "#     df_info_prune = add_info(info_prune, 1 - pct_test, df_info_prune)\n",
        "\n",
        "#     save(df_info_train, 'train', pct_data)\n",
        "#     save(df_info_prune, 'prune', pct_data)\n",
        "\n",
        "\n",
        "def full_pct_data(pct_data, pct_test=0.2): #pct of full data\n",
        "    change_pct_data(pct_data) \n",
        "    df_info_train = pd.DataFrame()\n",
        "    df_info_prune = pd.DataFrame()\n",
        "\n",
        "    print(f'\\n Percentage of Whole data {pct_data*100}% Test data {pct_test*100}% \\n')\n",
        "\n",
        "    info_train, info_prune = do_train(pct_test)\n",
        "    \n",
        "    df_info_train = add_info(info_train, 1 - pct_test, df_info_train)\n",
        "    df_info_prune = add_info(info_prune, 1 - pct_test, df_info_prune)\n",
        "\n",
        "    # save(df_info_train, 'train', pct_data)\n",
        "    # save(df_info_prune, 'prune', pct_data)\n",
        "\n",
        "    # df_info.plot.scatter(x='percentage', y='accuracy')\n",
        "    return df_info_train, df_info_prune    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm-3NVYrdG2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_pct_data(p, ptest):\n",
        "    global x, y, dft, dfp\n",
        "    x, y = full_pct_data(p, ptest)\n",
        "    x['epochs'] = EPOCHS\n",
        "    x['pct_data'] = p\n",
        "    x['pct_test'] = ptest\n",
        "\n",
        "    y['epochs'] = EPOCHS_PRUNE\n",
        "    y['pct_data'] = p\n",
        "    y['pct_test'] = ptest\n",
        "\n",
        "    dft = dft.append(x, ignore_index=True, verify_integrity=True) #trained\n",
        "    dfp = dfp.append(y, ignore_index=True, verify_integrity=True)\n",
        "\n",
        "    print()\n",
        "    display('Trained', x)\n",
        "    display('Pruned', y)\n",
        "    print()\n",
        "    print('Training Acc. Diff', (x.accuracy[0] - x.val_accuracy[0])*100)\n",
        "    print('Pruned Acc. Diff', (y.accuracy[0] - y.val_accuracy[0])*100)\n",
        "    # display('Difference', x - y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0u8AEWEEDz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dft = pd.DataFrame()\n",
        "dfp = pd.DataFrame()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXHeW4BykROX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30\n",
        "EPOCHS_PRUNE = 30\n",
        "VERBOSE = 1\n",
        "SHOW = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Kx3HpQeKFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "# history = model.fit(train_images, train_labels, epochs=EPOCHS, \n",
        "#                     validation_data=(test_images, test_labels), verbose=VERBOSE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-b5BlRlMBaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "101046e1-101f-4afc-d5a8-8eb5cdf6956c"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude, ConstantSparsity\n",
        "\n",
        "pruning_params = {\n",
        "    'pruning_schedule': ConstantSparsity(0.1, 0), #target, begin step\n",
        "    'block_size': (1, 1),\n",
        "    'block_pooling_type': 'AVG'}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:199: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS-3nWNpeZHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "59bf858a-0267-48f1-f431-28bf870bbc36"
      },
      "source": [
        "model_for_pruning.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d ( (None, 30, 30, 32)        1762      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 15, 15, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 13, 13, 64)        36930     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 6, 6, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 4, 4, 64)          73794     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 1024)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 64)                131138    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                1292      \n",
            "=================================================================\n",
            "Total params: 244,919\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 122,349\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzse7wQPONlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    # Log sparsity and other metrics in Tensorboard.\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)]\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o2L4E--eb_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "4d7211c0-2b58-4ecc-ff7d-a75f164f1b3f"
      },
      "source": [
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=15, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 1.7701 - accuracy: 0.3391 - val_loss: 1.7272 - val_accuracy: 0.3700\n",
            "Epoch 2/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 1.6421 - accuracy: 0.4062 - val_loss: 1.7365 - val_accuracy: 0.3540\n",
            "Epoch 3/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 1.5194 - accuracy: 0.4496 - val_loss: 1.7674 - val_accuracy: 0.3840\n",
            "Epoch 4/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 1.4551 - accuracy: 0.4698 - val_loss: 1.6857 - val_accuracy: 0.4100\n",
            "Epoch 5/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 1.3262 - accuracy: 0.5213 - val_loss: 1.7165 - val_accuracy: 0.4000\n",
            "Epoch 6/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 1.2283 - accuracy: 0.5651 - val_loss: 1.6776 - val_accuracy: 0.4200\n",
            "Epoch 7/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 1.1223 - accuracy: 0.5891 - val_loss: 1.8025 - val_accuracy: 0.3980\n",
            "Epoch 8/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 1.0796 - accuracy: 0.6164 - val_loss: 1.7711 - val_accuracy: 0.4320\n",
            "Epoch 9/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 0.9235 - accuracy: 0.6702 - val_loss: 1.7899 - val_accuracy: 0.4460\n",
            "Epoch 10/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 0.8563 - accuracy: 0.6964 - val_loss: 1.9274 - val_accuracy: 0.4360\n",
            "Epoch 11/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 0.7948 - accuracy: 0.7251 - val_loss: 2.0324 - val_accuracy: 0.4100\n",
            "Epoch 12/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 0.7302 - accuracy: 0.7393 - val_loss: 1.9397 - val_accuracy: 0.4640\n",
            "Epoch 13/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 0.6568 - accuracy: 0.7760 - val_loss: 2.1310 - val_accuracy: 0.4420\n",
            "Epoch 14/15\n",
            "36/36 [==============================] - 1s 21ms/step - loss: 0.5834 - accuracy: 0.7964 - val_loss: 2.0445 - val_accuracy: 0.4540\n",
            "Epoch 15/15\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 0.5020 - accuracy: 0.8307 - val_loss: 2.1240 - val_accuracy: 0.4580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa3d0363e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i46VDIpgMya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "change_pct_data(1)\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=15, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}