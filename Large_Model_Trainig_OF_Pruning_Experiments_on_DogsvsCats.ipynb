{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Large Model Trainig OF Pruning Experiments-on-DogsvsCats",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malraharsh/Pruning-Experiments/blob/master/Large_Model_Trainig_OF_Pruning_Experiments_on_DogsvsCats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJwIonXEVJo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f36270d6-72b2-48c5-9520-3510beca720f"
      },
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "import datetime\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude, ConstantSparsity\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "SHOW = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 40kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 3.3MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█▏                              | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 71kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 92kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 102kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 112kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 122kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 133kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 143kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 153kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 163kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 174kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 184kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 194kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 204kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 215kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 225kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 235kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 245kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 256kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 266kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 276kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 286kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 10.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJo71bj08Rg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makedir(path):\n",
        "    os.makedirs(path, exist_ok=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNCQRehK2ssk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path_saved_model = '/content/drive/My Drive/Colab Notebooks/Pruning Experiments/training_cp-1000train, 2000val, 300epochs---0250.h5'\n",
        "\n",
        "# makedir('saved_models')\n",
        "# !cp '/content/drive/My Drive/Colab Notebooks/Pruning Experiments/training_cp-1000train, 2000val, 300epochs---0250.h5' 'saved_models'\n",
        "# drive.flush_and_unmount()\n",
        "# path_saved_model = '/content/saved_models/training_cp-1000train, 2000val, 300epochs---0250.h5'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L68f061j9hSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_train = 'logs/train/'\n",
        "log_prune = 'logs/prune/'\n",
        "\n",
        "makedir(log_train)\n",
        "makedir(log_prune)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcujQblxeibt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "77d3ca94-661a-4832-a477-2a464a6c0e13"
      },
      "source": [
        "#https://www.tensorflow.org/tutorials/images/classification\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITOzKXSXetVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "# train_dir = os.path.join(PATH, 'validation')\n",
        "# validation_dir = os.path.join(PATH, 'train')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "\n",
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCL4dwnffjya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eeec1013-0e11-4ce4-c3e3-cda061786247"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 15\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "# train_image_generator = ImageDataGenerator(\n",
        "#                     rescale=1./255,\n",
        "#                     rotation_range=45,\n",
        "#                     width_shift_range=.15,\n",
        "#                     height_shift_range=.15,\n",
        "#                     horizontal_flip=True,\n",
        "#                     zoom_range=0.5)\n",
        "\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
        "\n",
        "\n",
        "# train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "#                                                            directory=train_dir,\n",
        "#                                                            shuffle=True,\n",
        "#                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "#                                                            class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n",
        "\n",
        "\n",
        "\n",
        "# def change_pct_test():\n",
        "#     train_image_generator = ImageDataGenerator(validation_split=pct_test, \n",
        "#                         rescale=1./255,\n",
        "#                         rotation_range=45,\n",
        "#                         width_shift_range=.15,\n",
        "#                         height_shift_range=.15,\n",
        "#                         horizontal_flip=True,\n",
        "#                         zoom_range=0.5)\n",
        "\n",
        "\n",
        "#     train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "#                                                             directory=train_dir,\n",
        "#                                                             shuffle=True,\n",
        "#                                                             target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "#                                                             class_mode='binary',\n",
        "#                                                             subset='training')\n",
        "\n",
        "#     val_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "#                                                                 directory=validation_dir,\n",
        "#                                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "#                                                                 class_mode='binary',\n",
        "#                                                                 subset='validation')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oijzVlV7mYtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_model():\n",
        "\n",
        "    model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1)])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def get_cifar_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def get_cifar_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    # model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes\n",
        "\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB25l8eRC6oH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None):#train_images, train_labels, test_images, test_labels):\n",
        "\n",
        "    train_image_generator = ImageDataGenerator(validation_split=pct_test, \n",
        "                        rescale=1./255,\n",
        "                        rotation_range=45,\n",
        "                        width_shift_range=.15,\n",
        "                        height_shift_range=.15,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.5)\n",
        "\n",
        "\n",
        "    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                            directory=train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                            class_mode='binary',\n",
        "                                                            subset='training')\n",
        "\n",
        "    val_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                                directory=train_dir,\n",
        "                                                                target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                                class_mode='binary',\n",
        "                                                                subset='validation')\n",
        "\n",
        "\n",
        "    if model is None:\n",
        "        model = get_cifar_model() # get_train_model()\n",
        "\n",
        "    # log_dir = log_train + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_train, histogram_freq=1)\n",
        "\n",
        "    history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=train_data_gen.samples // batch_size,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=val_data_gen.samples // batch_size)\n",
        "    # callbacks=[tensorboard_callback])\n",
        "    \n",
        "    return model, history.history    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y16uzTwOj_pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prune(model=None):\n",
        "\n",
        "\n",
        "    train_image_generator = ImageDataGenerator(validation_split=pct_test, \n",
        "                        rescale=1./255,\n",
        "                        rotation_range=45,\n",
        "                        width_shift_range=.15,\n",
        "                        height_shift_range=.15,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.5)\n",
        "\n",
        "\n",
        "    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                            directory=train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                            class_mode='binary',\n",
        "                                                            subset='training')\n",
        "\n",
        "    val_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                                directory=train_dir,\n",
        "                                                                target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                                class_mode='binary',\n",
        "                                                                subset='validation')\n",
        "\n",
        "\n",
        "    if model is None:\n",
        "        model = get_train_model()\n",
        "\n",
        "\n",
        "    # Define model for pruning.\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': ConstantSparsity(pct_prune, 0), #target, begin step\n",
        "        'block_size': (1, 1),\n",
        "        'block_pooling_type': 'AVG'}\n",
        "\n",
        "    end_step = np.ceil(total_val / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "    pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                               final_sparsity=pct_prune,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)}\n",
        "\n",
        "\n",
        "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "    # `prune_low_magnitude` requires a recompile.\n",
        "    model_for_pruning.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    callbacks = [\n",
        "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "      tfmot.sparsity.keras.PruningSummaries(log_dir=log_prune),\n",
        "    ]\n",
        "\n",
        "    history = model_for_pruning.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=train_data_gen.samples // batch_size,\n",
        "    epochs=EPOCHS_PRUNE,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=val_data_gen.samples // batch_size,\n",
        "    callbacks=callbacks,\n",
        "    verbose=VERBOSE)\n",
        "\n",
        "    return model_for_pruning, history.history"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXHeW4BykROX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1\n",
        "EPOCHS_PRUNE = 2\n",
        "VERBOSE = 1\n",
        "SHOW = 0\n",
        "pct_prune = 0.05\n",
        "pct_test = 0.2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqTMdi79KWo7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc5ffea5-9e4d-4c15-cbd9-5125fce24d93"
      },
      "source": [
        "total_train, total_val"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npyS80v61WBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EPOCHS = 30\n",
        "# pct_test = 0.3\n",
        "# model, dt = train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7cpdKx71Nc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30\n",
        "pct_test = 0.2\n",
        "model, dt = train(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hth9LTJp2pHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b263b84-7eba-4afa-b29e-4b15af4dfcc9"
      },
      "source": [
        "EPOCHS = 30\n",
        "pct_test = 0.2\n",
        "model, dt = train(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "25/25 [==============================] - 14s 567ms/step - loss: 0.4041 - accuracy: 0.8019 - val_loss: 0.5517 - val_accuracy: 0.7188\n",
            "Epoch 2/30\n",
            "25/25 [==============================] - 14s 565ms/step - loss: 0.4091 - accuracy: 0.8056 - val_loss: 0.5659 - val_accuracy: 0.7005\n",
            "Epoch 3/30\n",
            "25/25 [==============================] - 14s 565ms/step - loss: 0.3964 - accuracy: 0.8100 - val_loss: 0.6359 - val_accuracy: 0.7214\n",
            "Epoch 4/30\n",
            "25/25 [==============================] - 14s 568ms/step - loss: 0.4004 - accuracy: 0.8150 - val_loss: 0.6299 - val_accuracy: 0.6901\n",
            "Epoch 5/30\n",
            "25/25 [==============================] - 14s 562ms/step - loss: 0.4062 - accuracy: 0.8069 - val_loss: 0.7532 - val_accuracy: 0.6250\n",
            "Epoch 6/30\n",
            "25/25 [==============================] - 14s 561ms/step - loss: 0.4083 - accuracy: 0.8075 - val_loss: 0.6632 - val_accuracy: 0.6589\n",
            "Epoch 7/30\n",
            "25/25 [==============================] - 14s 565ms/step - loss: 0.4033 - accuracy: 0.8131 - val_loss: 0.6584 - val_accuracy: 0.6589\n",
            "Epoch 8/30\n",
            "25/25 [==============================] - 14s 561ms/step - loss: 0.3833 - accuracy: 0.8100 - val_loss: 0.5888 - val_accuracy: 0.7448\n",
            "Epoch 9/30\n",
            "25/25 [==============================] - 14s 570ms/step - loss: 0.3923 - accuracy: 0.8119 - val_loss: 0.6109 - val_accuracy: 0.7318\n",
            "Epoch 10/30\n",
            "25/25 [==============================] - 14s 561ms/step - loss: 0.3969 - accuracy: 0.8056 - val_loss: 0.5418 - val_accuracy: 0.7161\n",
            "Epoch 11/30\n",
            "25/25 [==============================] - 14s 567ms/step - loss: 0.3887 - accuracy: 0.8200 - val_loss: 0.5439 - val_accuracy: 0.7161\n",
            "Epoch 12/30\n",
            "25/25 [==============================] - 14s 563ms/step - loss: 0.3639 - accuracy: 0.8281 - val_loss: 0.7216 - val_accuracy: 0.6224\n",
            "Epoch 13/30\n",
            "25/25 [==============================] - 14s 564ms/step - loss: 0.3442 - accuracy: 0.8431 - val_loss: 0.6054 - val_accuracy: 0.7161\n",
            "Epoch 14/30\n",
            "25/25 [==============================] - 14s 567ms/step - loss: 0.3713 - accuracy: 0.8231 - val_loss: 0.6785 - val_accuracy: 0.6719\n",
            "Epoch 15/30\n",
            "25/25 [==============================] - 14s 564ms/step - loss: 0.3539 - accuracy: 0.8256 - val_loss: 0.6206 - val_accuracy: 0.6823\n",
            "Epoch 16/30\n",
            "25/25 [==============================] - 14s 560ms/step - loss: 0.3729 - accuracy: 0.8375 - val_loss: 0.5526 - val_accuracy: 0.7448\n",
            "Epoch 17/30\n",
            "25/25 [==============================] - 14s 559ms/step - loss: 0.3468 - accuracy: 0.8319 - val_loss: 0.6155 - val_accuracy: 0.7500\n",
            "Epoch 18/30\n",
            "25/25 [==============================] - 14s 575ms/step - loss: 0.3853 - accuracy: 0.8150 - val_loss: 0.5513 - val_accuracy: 0.7500\n",
            "Epoch 19/30\n",
            "25/25 [==============================] - 14s 566ms/step - loss: 0.3412 - accuracy: 0.8475 - val_loss: 0.4946 - val_accuracy: 0.7318\n",
            "Epoch 20/30\n",
            "25/25 [==============================] - 14s 562ms/step - loss: 0.3384 - accuracy: 0.8400 - val_loss: 0.5330 - val_accuracy: 0.7526\n",
            "Epoch 21/30\n",
            "25/25 [==============================] - 14s 559ms/step - loss: 0.3247 - accuracy: 0.8462 - val_loss: 0.8425 - val_accuracy: 0.6953\n",
            "Epoch 22/30\n",
            "25/25 [==============================] - 14s 561ms/step - loss: 0.3405 - accuracy: 0.8444 - val_loss: 0.5270 - val_accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "25/25 [==============================] - 14s 557ms/step - loss: 0.3217 - accuracy: 0.8600 - val_loss: 0.8008 - val_accuracy: 0.6927\n",
            "Epoch 24/30\n",
            "25/25 [==============================] - 14s 553ms/step - loss: 0.3749 - accuracy: 0.8388 - val_loss: 0.5392 - val_accuracy: 0.7604\n",
            "Epoch 25/30\n",
            "25/25 [==============================] - 14s 558ms/step - loss: 0.3456 - accuracy: 0.8431 - val_loss: 0.5224 - val_accuracy: 0.7656\n",
            "Epoch 26/30\n",
            "25/25 [==============================] - 14s 556ms/step - loss: 0.3328 - accuracy: 0.8531 - val_loss: 0.5516 - val_accuracy: 0.7526\n",
            "Epoch 27/30\n",
            "25/25 [==============================] - 14s 549ms/step - loss: 0.3569 - accuracy: 0.8231 - val_loss: 0.7671 - val_accuracy: 0.6901\n",
            "Epoch 28/30\n",
            "25/25 [==============================] - 14s 554ms/step - loss: 0.3351 - accuracy: 0.8456 - val_loss: 0.5155 - val_accuracy: 0.7917\n",
            "Epoch 29/30\n",
            "25/25 [==============================] - 14s 548ms/step - loss: 0.3100 - accuracy: 0.8594 - val_loss: 0.5597 - val_accuracy: 0.7578\n",
            "Epoch 30/30\n",
            "25/25 [==============================] - 14s 558ms/step - loss: 0.3166 - accuracy: 0.8537 - val_loss: 0.6140 - val_accuracy: 0.7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyNUv2uYknwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "loss: 0.5314 - accuracy: 0.7147 - val_loss: 0.9066 - val_accuracy: 0.5260\n",
        "loss: 0.5672 - accuracy: 0.7024 - val_loss: 1.5961 - val_accuracy: 0.5000\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJd9ZiaL_WC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path_cp = f\"training_cp-{EPOCHS:04d}.h5\"\n",
        "# model.save(path_cp)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGAB1V_N4E_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.models.load_model(path_saved_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V5QFecX4YBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.evaluate(train_data_gen, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhp138k46h87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_cp = f\"training_cp-0.2-90.h5\"\n",
        "model.save(path_cp)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDt0iw3a765y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/training_cp-0.2-90.h5' '/content/drive/My Drive/Colab Notebooks/Pruning Experiments/training_cp-0.2-90.h5'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVJ_cENM7vgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# !cp '/content/drive/My Drive/Colab Notebooks/Pruning Experiments/training_cp-0.2-90.h5' /content/\n",
        "# drive.flush_and_unmount()\n",
        "model = tf.keras.models.load_model('/content/training_cp-0.2-90.h5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt38ND5q4MrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a9981186-97a3-4ee8-8941-8d6ca5da487f"
      },
      "source": [
        "model.evaluate(val_data_gen, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 3s 182ms/step - loss: 0.5361 - accuracy: 0.7800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5360532999038696, 0.7799999713897705]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTRaNRlNOE7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "92adcac7-2551-47bc-c73f-92dc1f1bc556"
      },
      "source": [
        "EPOCHS_PRUNE = 3\n",
        "pct_prune = 0.05\n",
        "mp, dp = prune(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:199: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "Epoch 1/3\n",
            "25/25 [==============================] - 17s 673ms/step - loss: 0.3223 - accuracy: 0.8606 - val_loss: 0.5376 - val_accuracy: 0.7630\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 17s 670ms/step - loss: 0.3145 - accuracy: 0.8594 - val_loss: 0.5370 - val_accuracy: 0.7786\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 17s 668ms/step - loss: 0.2875 - accuracy: 0.8763 - val_loss: 0.5798 - val_accuracy: 0.7682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f1rIkulOEni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e72fee60-c770-4855-ac92-5816254d8d8d"
      },
      "source": [
        "pct_prune = 0.1\n",
        "mp, dp = prune(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "25/25 [==============================] - 17s 687ms/step - loss: 0.3246 - accuracy: 0.8487 - val_loss: 0.4928 - val_accuracy: 0.7969\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 17s 672ms/step - loss: 0.3019 - accuracy: 0.8650 - val_loss: 0.7637 - val_accuracy: 0.7135\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 17s 666ms/step - loss: 0.2822 - accuracy: 0.8712 - val_loss: 0.5877 - val_accuracy: 0.7760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u43q93aJOjs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "cc52327c-9781-4bc5-a333-486820a32454"
      },
      "source": [
        "pct_prune = 0.2\n",
        "mp, dp = prune(model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "25/25 [==============================] - 17s 694ms/step - loss: 0.3099 - accuracy: 0.8519 - val_loss: 0.5184 - val_accuracy: 0.7552\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 17s 675ms/step - loss: 0.2902 - accuracy: 0.8694 - val_loss: 0.4141 - val_accuracy: 0.8047\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 17s 675ms/step - loss: 0.2773 - accuracy: 0.8744 - val_loss: 0.5754 - val_accuracy: 0.7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayyMHgfkOgAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "f6edda7c-a3ce-4991-c606-b06d8267fe6e"
      },
      "source": [
        "EPOCHS_PRUNE = 10\n",
        "pct_prune = 0.1\n",
        "mp, dp = prune(model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 18s 701ms/step - loss: 0.3043 - accuracy: 0.8644 - val_loss: 0.5624 - val_accuracy: 0.7292\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 17s 682ms/step - loss: 0.2783 - accuracy: 0.8750 - val_loss: 0.7054 - val_accuracy: 0.7396\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 17s 676ms/step - loss: 0.2727 - accuracy: 0.8763 - val_loss: 0.6001 - val_accuracy: 0.7552\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 17s 680ms/step - loss: 0.2694 - accuracy: 0.8813 - val_loss: 0.5466 - val_accuracy: 0.7604\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 17s 676ms/step - loss: 0.2821 - accuracy: 0.8656 - val_loss: 0.9704 - val_accuracy: 0.6276\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 17s 679ms/step - loss: 0.2819 - accuracy: 0.8731 - val_loss: 0.5981 - val_accuracy: 0.7292\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 17s 678ms/step - loss: 0.2449 - accuracy: 0.8906 - val_loss: 0.4932 - val_accuracy: 0.7786\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 17s 680ms/step - loss: 0.2415 - accuracy: 0.8944 - val_loss: 0.6323 - val_accuracy: 0.7240\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 17s 678ms/step - loss: 0.2850 - accuracy: 0.8775 - val_loss: 0.4703 - val_accuracy: 0.7865\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 17s 678ms/step - loss: 0.2416 - accuracy: 0.8956 - val_loss: 0.6799 - val_accuracy: 0.7083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1nhbwK4kHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSHRl054j14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp4UVzyCOfOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL2g3MF2l57a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir /content/logs/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej49WdmHvgm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir /content/logs/prune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72X2ZHg7Dg9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}